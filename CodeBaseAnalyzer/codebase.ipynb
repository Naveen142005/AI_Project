{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191a1ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('GROQ_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a126ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "import stat, os, shutil\n",
    "\n",
    "def get_repo(repo_url: str, path_url: str = './temp_repo'):\n",
    "\n",
    "    def remove_readonly(func, path, exc_info):\n",
    "        os.chmod(path, stat.S_IWRITE)\n",
    "        func(path)\n",
    "    \n",
    "    if os.path.exists(path_url):\n",
    "        shutil.rmtree(path_url, onexc = remove_readonly)\n",
    "    \n",
    "    print('Cloning the repo...')\n",
    "    \n",
    "    Repo.clone_from(repo_url, path_url, depth = 1)\n",
    "    print('Clone completed !!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d619ef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree_sitter import Parser, Language\n",
    "from pathlib import Path\n",
    "import tree_sitter_python as tspython\n",
    "import tree_sitter_javascript as tsjavascript\n",
    "import tree_sitter_typescript as tstypescript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a942617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree_sitter import Parser, Language\n",
    "import tree_sitter_python as tspython\n",
    "import tree_sitter_javascript as tsjavascript\n",
    "import tree_sitter_typescript as tstypescript\n",
    "import tree_sitter_c as tsc\n",
    "import tree_sitter_cpp as tscpp\n",
    "\n",
    "# Look up table. To setup the language grammar for the tree sitter.\n",
    "LANGUAGES = { \n",
    "    \".py\": Language(tspython.language()),\n",
    "    \".js\": Language(tsjavascript.language()),\n",
    "    \".jsx\": Language(tsjavascript.language()),\n",
    "    \".ts\": Language(tstypescript.language_typescript()),\n",
    "    \".tsx\": Language(tstypescript.language_tsx()),\n",
    "    \n",
    "    \n",
    "    \".c\": Language(tsc.language()),\n",
    "    \".h\": Language(tsc.language()), \n",
    "    \".cpp\": Language(tscpp.language()),\n",
    "    \".hpp\": Language(tscpp.language()),\n",
    "    \".cc\": Language(tscpp.language()),\n",
    "    \".cxx\": Language(tscpp.language()),\n",
    "    \".hh\": Language(tscpp.language()),\n",
    "}\n",
    "\n",
    "# To specify the common name to the functions and classes for all languages.\n",
    "NODE_TYPES = {\n",
    "    \".py\": {\n",
    "        \"function\": \"function_definition\",\n",
    "        \"class\": \"class_definition\",\n",
    "    },\n",
    "    \".js\": {\n",
    "        \"function\": \"function_declaration\",\n",
    "        \"method\": \"method_definition\",\n",
    "        \"arrow\": \"arrow_function\",\n",
    "        \"class\": \"class_declaration\",\n",
    "    },\n",
    "    \".ts\": {\n",
    "        \"function\": \"function_declaration\",\n",
    "        \"method\": \"method_definition\",\n",
    "        \"arrow\": \"arrow_function\",\n",
    "        \"class\": \"class_declaration\",\n",
    "        \"interface\": \"interface_declaration\"\n",
    "    },\n",
    "    \".tsx\": {\n",
    "        \"function\": \"function_declaration\",\n",
    "        \"method\": \"method_definition\",\n",
    "        \"arrow\": \"arrow_function\",\n",
    "        \"class\": \"class_declaration\",\n",
    "    },\n",
    "    \n",
    "  \n",
    "    \".c\": {\n",
    "        \"function\": \"function_definition\",\n",
    "        \"struct\": \"struct_specifier\", \n",
    "        \"typedef\": \"type_definition\"\n",
    "    },\n",
    "    \".h\": {\n",
    "        \"function\": \"function_definition\",\n",
    "        \"struct\": \"struct_specifier\",\n",
    "        \"typedef\": \"type_definition\"\n",
    "    },\n",
    "\n",
    "   \n",
    "    \".cpp\": {\n",
    "        \"function\": \"function_definition\",\n",
    "        \"class\": \"class_specifier\",\n",
    "        \"struct\": \"struct_specifier\",\n",
    "        \"template\": \"template_declaration\"\n",
    "    },\n",
    "    \".hpp\": {\n",
    "        \"function\": \"function_definition\",\n",
    "        \"class\": \"class_specifier\",\n",
    "        \"struct\": \"struct_specifier\",\n",
    "        \"template\": \"template_declaration\"\n",
    "    },\n",
    "\n",
    "    \".cc\": { \"function\": \"function_definition\", \"class\": \"class_specifier\" },\n",
    "    \".cxx\": { \"function\": \"function_definition\", \"class\": \"class_specifier\" },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ddcec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_name(node, source):\n",
    "    \n",
    "    name_node = node.child_by_field_name('name')\n",
    "    \n",
    "   \n",
    "    if not name_node:\n",
    "        declarator = node.child_by_field_name('declarator')\n",
    "        \n",
    "       \n",
    "        while declarator:\n",
    "            \n",
    "            if declarator.type in ['identifier', 'field_identifier', 'type_identifier']:\n",
    "                name_node = declarator\n",
    "                break\n",
    "            \n",
    "            \n",
    "            next_decl = declarator.child_by_field_name('declarator')\n",
    "            if next_decl:\n",
    "                declarator = next_decl\n",
    "            else:\n",
    "              \n",
    "                name_node = declarator\n",
    "                break\n",
    "\n",
    "    if name_node:\n",
    "        return source[name_node.start_byte:name_node.end_byte].decode('utf8', errors='replace')\n",
    "    \n",
    "    return None\n",
    "\n",
    "def walk(node, results, ext, source):\n",
    "   \n",
    "    node_map = NODE_TYPES.get(ext, {})\n",
    "    \n",
    "    target_type = None\n",
    "    \n",
    "    # Check if this node matches one of our target types (function, class, etc.)\n",
    "    for common_name, type_name in node_map.items():\n",
    "        if node.type == type_name:\n",
    "            target_type = common_name\n",
    "            break\n",
    "        \n",
    "    if target_type:\n",
    "       \n",
    "        name = get_node_name(node, source)\n",
    "        \n",
    "        # Fallback if still not found\n",
    "        if not name:\n",
    "             name = f\"<anonymous_{target_type}_L{node.start_point[0]}>\"\n",
    "             \n",
    "        code = source[node.start_byte:node.end_byte].decode('utf8', errors=\"replace\")\n",
    "        \n",
    "        results.append({\n",
    "            'type': target_type,\n",
    "            'name': name,\n",
    "            'code': code,\n",
    "            'start_line': node.start_point[0] + 1,\n",
    "            'end_line': node.end_point[0] + 1\n",
    "        })\n",
    "\n",
    "    # Continue traversing children\n",
    "    for child in node.children:\n",
    "        walk(child, results, ext, source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2f0ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(path_url):\n",
    "    \"\"\"Reads the files and returns the chunks such functions/classes\"\"\"\n",
    "    ext = Path(path_url).suffix\n",
    "    \n",
    "    if ext not in LANGUAGES:\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        tree_parser = Parser(LANGUAGES[ext])\n",
    "        source = Path(path_url).read_bytes()\n",
    "        tree = tree_parser.parse(source)\n",
    "        \n",
    "        results = []\n",
    "        # print_tree(tree.root_node, source)\n",
    "        walk(tree.root_node, results, ext, source)\n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb4f335",
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_DIRS = {\n",
    "    \".git\",\n",
    "    \"__pycache__\",\n",
    "    \"node_modules\",\n",
    "    \"venv\",\n",
    "    \".venv\",\n",
    "    \"env\",\n",
    "    \"dist\",\n",
    "    \"build\",\n",
    "    \".idea\",\n",
    "    \".vscode\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dc5e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx, matplotlib.pyplot as plt\n",
    "\n",
    "graph = nx.MultiDiGraph()\n",
    "function_map = {}\n",
    "all_chunks = []\n",
    "global_bm25 = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a24bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(repo_path:str):\n",
    "    print('Building graph for your code base...')\n",
    "    # print(repo_path)\n",
    "    for root, dir, files in os.walk(repo_path):\n",
    "        dir[:] = [d for d in dir if d not in IGNORE_DIRS]\n",
    "        \n",
    "        for file in files: \n",
    "            full_path = os.path.join(root, file)\n",
    "            print(full_path)\n",
    "            \n",
    "            chunks = parse_file(full_path)\n",
    "            \n",
    "            for chunk in chunks:\n",
    "                func_name = chunk['name']\n",
    "                node_id = full_path + '::' +func_name\n",
    "                chunk['file_path'] = full_path\n",
    "                graph.add_node(\n",
    "                    node_id,\n",
    "                    name = chunk['name'],\n",
    "                    code = chunk['code'],\n",
    "                    type = chunk['type']\n",
    "                )\n",
    "                \n",
    "                function_map[func_name] = node_id\n",
    "                all_chunks.append(chunk)\n",
    "                \n",
    "        print('Linking the function calls...')\n",
    "        print(all_chunks)\n",
    "        \n",
    "        for chunk in all_chunks:\n",
    "            caller_name = chunk['name']\n",
    "            caller_code = chunk['code']\n",
    "            \n",
    "            for target_name, target_id in function_map.items():\n",
    "                if (target_name == caller_name): continue\n",
    "\n",
    "                if target_name in caller_code:\n",
    "                    graph.add_edge(caller_name, target_id)\n",
    "                    print(f\"   -> {caller_name} calls {target_id}\")\n",
    "                    \n",
    "    print('The Graph has been built successFully !!')          \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00f13bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_graph(repo_path='./temp_repo')\n",
    "# nx.draw_forceatlas2(graph, with_labels=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb2b957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "#Load the Model (Base Code)\n",
    "model_id = \"jinaai/jina-embeddings-v2-base-code\" \n",
    "print(f\"Loading model: {model_id}...\")\n",
    "\n",
    "model = AutoModel.from_pretrained(model_id, trust_remote_code=True)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Base Code Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8faa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(chunks):\n",
    "    if not chunks:\n",
    "        return []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embeddings = model.encode(chunks)\n",
    "        \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472705fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "chromadb_client = chromadb.PersistentClient('./chromadb')\n",
    "collection = chromadb_client.get_or_create_collection('codebase-vectors')\n",
    "print ('Chromadb\\'s setup successfully !!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68c203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# We need a place to map the BM25 results back to the original chunk data\n",
    "bm25_mapping = {} \n",
    "\n",
    "def create_bm25_index(chunks):\n",
    "    \"\"\"\n",
    "    Input: List of code chunk dictionaries\n",
    "    Action: Builds a searchable keyword index\n",
    "    \"\"\"\n",
    "    tokenized_corpus = []\n",
    "    \n",
    "    print(f\"ðŸ”¤ Building BM25 Index for {len(chunks)} items...\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Search against the function NAME and the CODE\n",
    "        text = chunk['name'] + \" \" + chunk['code']\n",
    "        \n",
    "        \n",
    "        tokens = text.lower().split()\n",
    "        tokenized_corpus.append(tokens)\n",
    "        \n",
    "       \n",
    "        bm25_mapping[i] = chunk\n",
    "        \n",
    "    # Create the Index\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    return bm25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129df39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_codebase(chunks, vectors):\n",
    "   \n",
    "    print(f\" Indexing {len(chunks)} items...\")\n",
    "    \n",
    "    ids = [str(i) for i in range(len(chunks))] \n",
    "    metadatas = []\n",
    "    documents = [] \n",
    "    \n",
    "    for chunk in chunks:\n",
    "        metadatas.append({\n",
    "            \"name\": chunk['name'],\n",
    "            \"file_path\": chunk['file_path'],\n",
    "            \"type\": chunk['type'],\n",
    "           \n",
    "            \"start_line\": int(chunk['start_line']),\n",
    "            \"end_line\": int(chunk['end_line'])\n",
    "        })\n",
    "        documents.append(chunk['code'])\n",
    "\n",
    "    collection.add(\n",
    "        ids=ids,\n",
    "        embeddings=vectors,\n",
    "        metadatas=metadatas,\n",
    "        documents=documents\n",
    "    )\n",
    "    print(\"   -> Vector Store Updated.\")\n",
    "    \n",
    "    bm25_index = create_bm25_index(chunks)\n",
    "    print(\"   -> Keyword Index Built.\")\n",
    "    \n",
    "    return bm25_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bbad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def ingest_repository(github_url):\n",
    "    \"\"\"\n",
    "    Master function to load a new repo and prepare the RAG system.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"INITIALIZING NEW REPO: {github_url}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    get_repo(github_url, path_url='./temp_repo')\n",
    "\n",
    "  \n",
    "    global graph, function_map, all_chunks\n",
    "    graph = nx.MultiDiGraph()\n",
    "    function_map = {}\n",
    "    all_chunks = []\n",
    "    \n",
    "   \n",
    "    build_graph(repo_path='./temp_repo')\n",
    "    \n",
    "    if not all_chunks:\n",
    "        print(\"Warning: No code found! Check if the repo path is correct.\")\n",
    "        return\n",
    "\n",
    "   \n",
    "    print(f\"Embedding {len(all_chunks)} functions...\")\n",
    "    codes = [c['code'] for c in all_chunks]\n",
    "    vectors = get_embeddings(codes)\n",
    "\n",
    "    global global_bm25 \n",
    "    global_bm25 = index_codebase(all_chunks, vectors)\n",
    "\n",
    "    print(f\"\\nSUCCESS! Repository loaded. The Agent is ready to answer questions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff49dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_results(results_a, results_b):\n",
    "    \n",
    "    seen_ids = set()\n",
    "    merged = []\n",
    "    \n",
    "    # Combine both lists\n",
    "    all_results = results_a + results_b\n",
    "    \n",
    "    for chunk in all_results:\n",
    "        # Create a unique signature for this chunk\n",
    "        unique_id = f\"{chunk['file_path']}::{chunk['name']}\"\n",
    "        \n",
    "        if unique_id not in seen_ids:\n",
    "            merged.append(chunk)\n",
    "            seen_ids.add(unique_id)\n",
    "            \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6c08d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query, k = 5):\n",
    "    print (f'Searching for the given {query}')\n",
    "    \n",
    "    #1) Vector search\n",
    "    query_vector = get_embeddings([query])[0]\n",
    "    chroma_results = collection.query(\n",
    "        query_embeddings=query_vector,\n",
    "        n_results=k\n",
    "    )\n",
    "    \n",
    "    #formating the chromo results\n",
    "    vector_hits = []\n",
    "    if chroma_results['ids']:\n",
    "        for i in range (len(chroma_results[\"ids\"][0])):\n",
    "            meta = chroma_results['metadatas'][0][i]\n",
    "            doc = chroma_results['documents'][0][i]\n",
    "            vector_hits.append({\n",
    "                \"name\": meta['name'],\n",
    "                \"file_path\": meta['file_path'],\n",
    "                \"code\": doc,\n",
    "                \"score\": chroma_results['distances'][0][i],\n",
    "                \"source\": \"vector\"\n",
    "            })\n",
    "    \n",
    "    # 2) Keyword Search (BM25)\n",
    "    tokenized_query = query.lower().split()\n",
    "    bm25_scores = global_bm25.get_scores(tokenized_query)\n",
    "    top_n = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:k]\n",
    "    \n",
    "    keyword_hits = []\n",
    "    for idx in top_n:\n",
    "        # Filter out results with 0 score (no keyword match)\n",
    "        if bm25_scores[idx] > 0:\n",
    "            original_chunk = bm25_mapping[idx] # Look up the original data\n",
    "            keyword_hits.append({\n",
    "                \"name\": original_chunk['name'],\n",
    "                \"file_path\": original_chunk['file_path'],\n",
    "                \"code\": original_chunk['code'],\n",
    "                \"score\": bm25_scores[idx],\n",
    "                \"source\": \"keyword\"\n",
    "            })\n",
    "    final_results = deduplicate_results(vector_hits, keyword_hits)\n",
    "    \n",
    "    return final_results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337dcb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_context(initial_results, graph, max_depth=1):\n",
    "    expanded_results = []\n",
    "    seen = set()\n",
    "   \n",
    "    for x in initial_results:\n",
    "        node_id = f\"{x['file_path']}::{x['name']}\"\n",
    "        if node_id not in seen:\n",
    "            expanded_results.append(x)\n",
    "            seen.add(node_id)\n",
    "    \n",
    "    \n",
    "    for x in initial_results:\n",
    "        node_id = f\"{x['file_path']}::{x['name']}\"\n",
    "        \n",
    "        if node_id in graph:\n",
    "            neighbors = list(graph.successors(node_id))\n",
    "            if neighbors:\n",
    "                print(f\" '{x['name']}' calls: {len(neighbors)} dependencies\")\n",
    "                \n",
    "                for neighbor_id in neighbors:\n",
    "                    if neighbor_id not in seen:\n",
    "                        data = graph.nodes[neighbor_id]\n",
    "                        name = neighbor_id.split('::')[-1]\n",
    "                        \n",
    "                        neighbor_chunk = {\n",
    "                            \"name\": name,\n",
    "                            \"file_path\": data.get('file', 'unknown'),\n",
    "                            \"code\": data.get('code', ''),\n",
    "                            \"type\": \"dependency\", \n",
    "                            \"score\": 0.0,\n",
    "                            \"source\": \"graph\"\n",
    "                        }\n",
    "                    expanded_results.append(neighbor_chunk)\n",
    "                    seen.add(neighbor_id)\n",
    "    print(f\"Context grew from {len(initial_results)} to {len(expanded_results)} chunks.\")\n",
    "    return expanded_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370d1a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "\n",
    "def rerank_results(query: str,chunks: list,top_k: int = 5,max_rerank: int = 200):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      - query: User question\n",
    "      - chunks: Results after Hybrid Search + Graph Expansion\n",
    "      - top_k: Final number of chunks to keep\n",
    "      - max_rerank: Safety cap for CrossEncoder (performance + quality)\n",
    "\n",
    "    Output:\n",
    "      - Top-k chunks sorted by rerank_score\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Reranking {len(chunks)} chunks...\")\n",
    "\n",
    "    if not chunks:\n",
    "        return []\n",
    "\n",
    "  \n",
    "    candidates = chunks[:max_rerank]\n",
    "\n",
    " \n",
    "    pairs = []\n",
    "    for c in candidates:\n",
    "       \n",
    "        structured_code = (\n",
    "            f\"Type: {c.get('type', 'code')}\\n\"\n",
    "            f\"File: {c.get('file_path', 'unknown')}\\n\\n\"\n",
    "            f\"{c.get('code', '')}\"\n",
    "        )\n",
    "        pairs.append([query, structured_code])\n",
    "\n",
    "\n",
    "    scores = reranker.predict(pairs)\n",
    "\n",
    "    for i, score in enumerate(scores):\n",
    "        final_score = float(score)\n",
    "\n",
    "       \n",
    "        if candidates[i].get(\"source\") == \"graph\":\n",
    "            final_score *= 0.9\n",
    "\n",
    "        candidates[i][\"rerank_score\"] = final_score\n",
    "\n",
    "\n",
    "    candidates.sort(key=lambda x: x[\"rerank_score\"],reverse=True)\n",
    "\n",
    "    final_results = candidates[:top_k]\n",
    "\n",
    "    print(\n",
    "        f\"Kept top {len(final_results)} chunks \"\n",
    "        f\"(Best Score: {final_results[0]['rerank_score']:.4f})\"\n",
    "    )\n",
    "\n",
    "    return final_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65864b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "def route_question(state):\n",
    "    question = state[\"question\"]\n",
    "    print(f\"Routing Query: '{question}'\")\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "    You are an intelligent router for a RAG system.\n",
    "    \n",
    "    CRITICAL RULE:\n",
    "    If the user asks a technical question (about code, logic, functions, files) mixed with a greeting, YOU MUST CHOOSE 'retrieve'.\n",
    "    \n",
    "    Examples:\n",
    "    - \"Hi, how does login work?\" -> retrieve (Technical intent exists)\n",
    "    - \"Hello!\" -> direct_answer (Pure greeting)\n",
    "    - \"Thanks, but where is the main file?\" -> retrieve (Technical intent exists)\n",
    "    - \"What is the weather?\" -> direct_answer (Not about code)\n",
    "    \n",
    "    Answer with ONE WORD ONLY: 'retrieve' or 'direct_answer'.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama-3.1-8b-instant\", \n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ],\n",
    "            temperature=0, \n",
    "            max_tokens=10 \n",
    "        )\n",
    "\n",
    "        decision = response.choices[0].message.content.strip().lower()\n",
    "        \n",
    "        if \"retrieve\" in decision:\n",
    "            print(f\" Decision: RETRIEVE (Database Search)\")\n",
    "            return \"retrieve\"\n",
    "        else:\n",
    "            print(f\" Decision: DIRECT ANSWER (Chitchat)\")\n",
    "            return \"direct_answer\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Router Error: {e}\")\n",
    "        return \"retrieve\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00400b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_documents(state):\n",
    "    print(\"GRADER: Filtering documents...\")\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    kept_docs = []\n",
    "\n",
    "\n",
    "    for doc in documents:\n",
    "        prompt = f\"\"\"\n",
    "    You are a strict relevance grader for a codebase question-answering system.\n",
    "\n",
    "    TASK:\n",
    "    Decide whether the given code snippet is useful for answering the user's question.\n",
    "\n",
    "    USER QUESTION:\n",
    "    {question}\n",
    "\n",
    "    CODE SNIPPET:\n",
    "    {doc['code'][:1000]}\n",
    "\n",
    "    DECISION RULES:\n",
    "    - Answer \"yes\" ONLY if the code directly helps explain, implement, or understand the question.\n",
    "    - Answer \"no\" if the code is unrelated, generic, or does not help answer the question.\n",
    "    - Do NOT explain your decision.\n",
    "    - Do NOT add extra text.\n",
    "\n",
    "    Answer ONLY one word: yes or no\n",
    "\n",
    "    \"\"\"\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama-3.1-8b-instant\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        decision = response.choices[0].message.content.strip().lower()\n",
    "\n",
    "        if \"yes\" in decision:\n",
    "            print(f\"   KEEP: {doc['name']}\")\n",
    "            kept_docs.append(doc)\n",
    "        else:\n",
    "            print(f\"   DROP: {doc['name']}\")\n",
    "\n",
    "    return {\n",
    "        \"documents\": kept_docs,\n",
    "        \"question\": question\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fb9d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "    \n",
    "    Attributes:\n",
    "        question: The user's original question\n",
    "        generation: The final answer\n",
    "        documents: List of code chunks found so far\n",
    "        search_count: Safety counter to prevent infinite loops\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[dict] \n",
    "    search_count: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc92a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using the filtered documents.\n",
    "    \"\"\"\n",
    "    print(\"GENERATOR: Writing final answer...\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # Format context for the LLM\n",
    "    context_text = \"\"\n",
    "    if documents:\n",
    "        for doc in documents:\n",
    "            context_text += f\"\\nFile: {doc.get('file_path', 'unknown')}\\nCode:\\n{doc['code']}\\n{'-'*20}\"\n",
    "    else:\n",
    "        context_text = \"No relevant code found.\"\n",
    "        \n",
    "    prompt = f\"\"\"\n",
    "    You are a Senior Software Engineer. Use the provided code context to answer the user's question.\n",
    "    \n",
    "    RULES:\n",
    "    1. Base your answer ONLY on the context.\n",
    "    2. Cite file names and function names.\n",
    "    3. Be concise and technical.\n",
    "    4. If the code doesn't answer the question, admit it.\n",
    "    \n",
    "    QUESTION: {question}\n",
    "    \n",
    "    CONTEXT:\n",
    "    {context_text}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\", \n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    return {\"generation\": response.choices[0].message.content}\n",
    "\n",
    "\n",
    "def retrieve_node(state):\n",
    "    \"\"\"\n",
    "    Orchestrates the Retrieval Pipeline: Search -> Expand -> Rerank\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    print(f\"RETRIEVER: Processing '{question}'\")\n",
    "    \n",
    "    # 1. Search (Hybrid)\n",
    "    raw_docs = hybrid_search(question, k=5)\n",
    "    \n",
    "    # 2. Expand (Graph)\n",
    "    expanded_docs = expand_context(raw_docs, graph)\n",
    "    \n",
    "    # 3. Rerank (Cross Encoder)\n",
    "    reranked_docs = rerank_results(question, expanded_docs, top_k=5, max_rerank=50)\n",
    "    \n",
    "    return {\"documents\": reranked_docs}\n",
    "\n",
    "def direct_answer_node(state):\n",
    "    \"\"\"\n",
    "    Handles simple chitchat.\n",
    "    \"\"\"\n",
    "    print(\"DIRECT ANSWER: Handling chitchat...\")\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful coding assistant. Answer the user's greeting or general question politely.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "    )\n",
    "    return {\"generation\": response.choices[0].message.content}\n",
    "\n",
    "# --- 3. Build the LangGraph ---\n",
    "\n",
    "print(\"Building Graph...\")\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Add Nodes\n",
    "workflow.add_node(\"retrieve\", retrieve_node)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"direct_answer\", direct_answer_node)\n",
    "\n",
    "# Add Conditional Entry Point (The Router)\n",
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"retrieve\": \"retrieve\",\n",
    "        \"direct_answer\": \"direct_answer\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Add Edges (The Flow)\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "\n",
    "# 2. After Grading, go to Generation\n",
    "workflow.add_edge(\"grade_documents\", \"generate\")\n",
    "\n",
    "# 3. End points\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"direct_answer\", END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()\n",
    "print(\"Agent Compiled Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998b7426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run the agent nicely\n",
    "def ask_agent(query):\n",
    "    inputs = {\"question\": query}\n",
    "    \n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"USER QUERY: {query}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    final_answer = \"\"\n",
    "    \n",
    "    # Stream the steps so we see what's happening\n",
    "    for output in app.stream(inputs):\n",
    "        for key, value in output.items():\n",
    "            print(f\"   Shape Shift -> Finished Node: {key}\")\n",
    "            if \"generation\" in value:\n",
    "                final_answer = value[\"generation\"]\n",
    "\n",
    "    print(f\"\\nFINAL ANSWER:\\n{final_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607df4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = input(\"Enter GitHub URL: \")\n",
    "ingest_repository(url) \n",
    "\n",
    "\n",
    "print(\"System Ready! Ask away...\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    if user_input.lower() in [\"quit\", \"exit\"]:\n",
    "        break\n",
    "        \n",
    "    ask_agent(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057b3a03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_ENV (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
